---
title: "Expectation Maximization Algorithm"
author: "Kevin Liu"
date: "7/25/2018"
output: 
  bookdown::html_document2:
    toc: true
    toc_depth: 4
    self_contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, fig.align='center')
```

Expectation maximization (EM) algorithm is one of the most widely used algorithm in statistics. My first encounter with EM algorithm was in STAT8003 (Statistical Methods I) in Fall 2015 Temple University. Prof Zhao ,who taught this class, did a very good job explaining EM algorithm and shared all class notes with students. However, it still took me a week to fully understand EM algorithm. In this webpage, I show an example of EM algorithm, from homework of STAT8003, in details. I hope both my wife and I will benefit from this in our future studies. 

# Derivation of Formulas for a simulated dataset

Assume that the data $x_1,\dots,x_n$ follows the following distribution:

\begin{equation}
x_i \sim f(x_i) = \pi_0 f_0(x_i) + \pi_1 f_1(x_i)
(\#eq:e1)
\end{equation}

where 

$f_0(x_i) = 1(0 \le x_i \le 1)$, which is the density function of uniform distribution

$f_1(x_i)=\beta(1-x_i)^{\beta-1}$, which is density function of Beta$(1,\beta)$

$z_i$, which is not shown in \@ref(eq:e1), is group information and should be treated as a missing value.

Define parameters that we are looking to solve to be 

\begin{equation}
\boldsymbol{\theta} = (\pi_0, \beta)
(\#eq:e2)
\end{equation}

One important relationship between $\pi_0$ and $\pi_1$ 

\begin{equation}
\pi_0 = 1 - \pi_1
(\#eq:e3)
\end{equation}

Note that 

\begin{equation}
\begin{split}
f(x_i,z_i ; \boldsymbol{\theta}) &= 
\mathbf{I}(z_i = 0 ; \boldsymbol{\theta}) \pi_0 f_0(x_i) + 
\mathbf{I}(z_i = 1 ; \boldsymbol{\theta}) \pi_1 f_0(x_i) \\
&=
\sum_{j=0}^{1} 
\mathbf{I}(z_i = j ; \boldsymbol{\theta}) \pi_j f_j(x_i)
\end{split}
(\#eq:e4)
\end{equation}

where $\mathbf{I}(z_i = j)$ is [indicator function](https://en.wikipedia.org/wiki/Indicator_function). Indicator function has following important property.

\begin{equation}
E(\mathbf{I}(z_i = j)) = P(z_i = j)
(\#eq:e5)
\end{equation}

From \@ref(eq:e4), we can derive the likelihood function

\begin{equation}
L(\boldsymbol{\theta};(x_1,\dots,x_n),(z_1,\dots,z_n)) = 
L(\boldsymbol{\theta};\mathbf{x},\mathbf{z}) = 
\prod_{i=1}^{n}
\sum_{j=0}^{1} 
\mathbf{I}(z_i = j ; \boldsymbol{\theta}) \pi_j f_j(x_i)
(\#eq:e6)
\end{equation}

and the log-likelihood is 

\begin{equation}
\begin{split}
\ell(\boldsymbol{\theta};\mathbf{x},\mathbf{z}) 
&=
log(\prod_{i=1}^{n}
\sum_{j=0}^{1} 
\mathbf{I}(z_i = j ; \boldsymbol{\theta}) \pi_j f_j(x_i)) \\
&=
\sum_{i=1}^{n}
log(\sum_{j=0}^{1} 
\mathbf{I}(z_i = j ; \boldsymbol{\theta}) \pi_j f_j(x_i)) \\
&=
\sum_{i=1}^{n}
\sum_{j=0}^{1} 
log(\mathbf{I}(z_i = j ; \boldsymbol{\theta}) \pi_j f_j(x_i))
\end{split}
(\#eq:e7)
\end{equation}

Given the $t$-th iteration of parameter $\boldsymbol{\theta}^{(t)}$, define

\begin{equation}
\begin{split}
T^{(t)}_{j,i} &= 
P(Z_i = j| \mathbf{x}; \boldsymbol{\theta}^{(t)})\\
&=
\frac{\pi_j^{(t)} f_j (x_i)}{\pi_0^{(t)} f_0 (x_i) + \pi_1^{(t)} f_1 (x_i) }
\end{split}
(\#eq:e8)
\end{equation}

The E step: derive expectation of log-likelihood. We define 

\begin{equation}
\begin{split}
Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)}) &=
E_{\mathbf{z};\mathbf{x},\boldsymbol{\theta}^{(t)}}(\ell(\boldsymbol{\theta};\mathbf{x},\mathbf{z}))\\
&\stackrel{(1.7)}{=}
E_{\mathbf{z};\mathbf{x},\boldsymbol{\theta}^{(t)}}(
\sum_{i=1}^{n}
\sum_{j=0}^{1} 
log(\mathbf{I}(z_i = j ;\mathbf{x}, \boldsymbol{\theta}) \pi_j f_j(x_i))
)\\
&=
\sum_{i=1}^{n}
\sum_{j=0}^{1} 
E_{\mathbf{z};\mathbf{x},\boldsymbol{\theta}^{(t)}}(
log(\mathbf{I}(z_i = j ;\mathbf{x}, \boldsymbol{\theta}) \pi_j f_j(x_i))
)\\
&=
\sum_{i=1}^{n}
\sum_{j=0}^{1} 
E_{\mathbf{z};\mathbf{x},\boldsymbol{\theta}^{(t)}}(\mathbf{I}(z_i = j ;\mathbf{x}, \boldsymbol{\theta})
log(
\pi_j f_j(x_i)
))\\
&=
\sum_{i=1}^{n}
\sum_{j=0}^{1} 
E_{\mathbf{z};\mathbf{x},\boldsymbol{\theta}^{(t)}}(
\mathbf{I}(z_i = j ;\mathbf{x}, \boldsymbol{\theta}^{(t)})
)

log(\pi_j f_j(x_i))\\
&\stackrel{(1.5)}{=}
\sum_{i=1}^{n}
\sum_{j=0}^{1}
P(z_i = j ; \mathbf{x}, \boldsymbol{\theta}^{(t)})
log(\pi_j f_j(x_i))\\
&=
\sum_{i=1}^{n}
\sum_{j=0}^{1}
P(z_i = j | \mathbf{x}; \boldsymbol{\theta}^{(t)})
log(\pi_j f_j(x_i))\\
&=
\sum_{i=1}^{n}
P(z_i = 0 | \mathbf{x}; \boldsymbol{\theta}^{(t)})
log(\pi_0 f_0(x_i)) +
P(z_i = 1 | \mathbf{x}; \boldsymbol{\theta}^{(t)})
log(\pi_1 f_1(x_i))\\
&\stackrel{(1.8)}{=}
\sum_{i=1}^{n}
T^{(t)}_{0,i}
log(\pi_0 f_0(x_i))
+
T^{(t)}_{1,i}
log(\pi_1 f_1(x_i))\\
&=
\sum_{i=1}^{n}
T^{(t)}_{0,i}
log(\pi_0 \times 1)
+
T^{(t)}_{1,i}
log(\pi_1 \times \beta(1-x_i)^{\beta-1})\\
&=
\sum_{i=1}^{n}
T^{(t)}_{0,i}
log(\pi_0)
+
T^{(t)}_{1,i}
(log(\pi_1 ) + log(\beta) + (\beta-1)log(1-x_i))
\end{split}
(\#eq:e9)
\end{equation}

The M step: maximize $Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)})$

From calculus, we know that first derivatives must be 0. (Note: $T^{(t)}_{j,i}$ should be treated as constant)

\begin{equation}
\frac{\partial Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)})}{\partial \pi_0} =
\sum_{i=1}^{n}
T^{(t)}_{0,i}
\frac{1}{\pi_0}
+
\sum_{i=1}^{n}
T^{(t)}_{1,i}
\frac{1}{\pi_0 - 1} = 0
(\#eq:e10)
\end{equation}

This implies

\begin{equation}
\pi_0^{(t+1)} = \frac{\sum_{i=1}^{n}
T^{(t)}_{0,i}}{\sum_{i=1}^{n}
T^{(t)}_{0,i} + \sum_{i=1}^{n}
T^{(t)}_{1,i}}
=
\frac{\sum_{i=1}^{n}
T^{(t)}_{0,i}}{\sum_{i=1}^{n} (
T^{(t)}_{0,i} + T^{(t)}_{1,i})}
= \frac{\sum_{i=1}^{n}
T^{(t)}_{0,i}}{n}
(\#eq:e11)
\end{equation}

Similarly, we have that 

\begin{equation}
\frac{\partial Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)})}{\partial \beta} = 
\sum_{i=1}^{n}
T^{(t)}_{1,i}
(\frac{1}{\beta} + log(1-x_i)) = 0
(\#eq:e12)
\end{equation}

This implies

\begin{equation}
\beta^{(t+1)} = \frac{\sum_{i=1}^{n}
T^{(t)}_{1,i}}{-\sum_{i=1}^{n}
T^{(t)}_{1,i} log(1-x_i)}
(\#eq:e13)
\end{equation}

Here is what I learn from this long calculation. For EM algorithm, we can follow those steps. 

1. Derive the complete log-likelihood function.
2. Derive the expectation of log-likelihood function, with respect to the current conditional distribution of random variable $z|\mathbf{x},\boldsymbol{\theta}^{(t)}$. Where $\mathbf{x}$ is actual data/value, $\boldsymbol{\theta}^{(t)}$ are parameters estimation at t-th iteration. **E step**.
3. Choose initial values for $\boldsymbol{\theta}^{(0)}$
4. Find the parameters that maximize $\boldsymbol{\theta}^{(1)} = \text{argmax } Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(0)})$. This is first iteration. **M step**.
5. Repeat step 4 until convergence. $\boldsymbol{\theta}^{(t+1)} = \text{argmax } Q(\boldsymbol{\theta}|\boldsymbol{\theta}^{(t)})$.

# Calculation by `R`

Create a loop in `R` to get $\pi_0$ and $\beta$.
With initial value of $0.69$ and $11$, at 32th iteration, $\pi_0 = 0.69679$ and $\beta = 11.09328$. (Initial value are chosen on purpose so it doesn't require too many iterations until converge).

```{r}
pvalue <- read.csv("files/pvalue.csv")

em <- function(X,s) {
  T0.all <- s[1]/ (s[1]+(1-s[1])*s[2]*(1-X)^(s[2]-1))
  s[1] <- mean(T0.all)
  s[2] <- -sum(1-T0.all)/sum((1-T0.all)*(log(1-X)))
  return(s)
}

s.old <- c(0.69, 11)
s.new <- s.old
delta <- 0.0001
Delta <- 1
ITR <- 1
while( Delta > delta ){
  s.new <- em(X = pvalue$X, s.old)
  Delta <- sum( (s.new-s.old)^2 )
  Delta <- max( abs(s.new-s.old) )
  ITR <- ITR+1
  s.old <- s.new
  print( paste(ITR, "-th iteration: pi0=", s.new[1], ", beta=", s.new[2] ) )
}
```

# Histogram
```{r, fig.align = 'center'}
library(ggplot2)
pi0 <- s.new[1]
pi1 <- 1-pi0
beta <- s.new[2]
hist.plot <- hist(pvalue$X, br=40, plot = FALSE)
df.plot <- data.frame(x = hist.plot$breaks[-1], y = hist.plot$density)
list.density <- density(pvalue$X)
df.plot2 <- data.frame(x = list.density$x, y = list.density$y)
df.plot2 <- subset(df.plot2, x >=0 & x <=1)
df.plot2["color"] <- "Gaussian Kernel Density Estimation"

df.plot3 <- data.frame(x = df.plot2$x)
df.plot3["y"] <- (pi0*1+ pi1*beta*(1-df.plot3$x)^(beta-1))
df.plot3["color"] <- "EM Estimation"

df.line <- rbind(df.plot2,df.plot3)
rm(df.plot2,df.plot3)

ggplot(data = df.plot, aes(x = x, y = y)) +
  geom_col(fill = "#B9CFE7", colour = "black") +
  geom_line(data = df.line ,aes(x = x, y = y, color = color),size = 1) +
  scale_y_continuous("density") +
  theme_bw() +
  scale_color_manual(values = c("#F0E442","#FF9999"), name = element_blank()) +
  theme(legend.position = "bottom")
```

From the histogram, we can easily see that EM density estimation fits the real density very well. Not surprise to us, EM density estimation is a better choice comparing with Gaussian kernel density estimation for this data set. 

# EM algorithm as a classification tool for mixture probabaility density. 

We can classify $x_i$ to be 1st group if $T_{0,i}=P(z_i=0|\mathbf{x};\boldsymbol{\theta}^{(\text{32th iteration})}) \stackrel{(1.8)}{=} \frac{0.69679 \times 1}{0.69679 \times 1 + (1-0.69679) \times 11.09327(1-x_i)^{(11.09327 -1)}} > 0.5$.

Use `R` to calculate

```{r}
pvalue["p0"] <- s.new[1]/(s.new[1] + (1-s.new[1]) * s.new[2] * (1-pvalue$X)^(s.new[2] - 1))
pvalue[pvalue$p0 >= 0.5, "estimated_group"] <- as.integer(0)
pvalue[pvalue$p0 < 0.5, "estimated_group"] <- as.integer(1)
pvalue[pvalue$group == pvalue$estimated_group,"Diff"] <- "Right classified"
pvalue[pvalue$group != pvalue$estimated_group,"Diff"] <- "False classified"
table(pvalue$Diff)
```
The total number of flase classified is $321$. The false classified rate is $321/(321+1679) = 0.1605$.